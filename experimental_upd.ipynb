{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os \n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import math\n",
    "import einops\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Any\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "plt.ioff()\n",
    "matplotlib.use('agg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes positional encoding as given in paper 'Attention is all you need'.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model : int, dropout_p : float, max_len : int) -> torch.Tensor:\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:, :token_embedding.size(1), :])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes embeddings for each patch in the image. \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size : int, n_channels : int, device : str, latent_size : int) -> torch.Tensor:\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.latent_size = latent_size\n",
    "        self.n_channels = n_channels\n",
    "        self.device = device\n",
    "        # Embedding size of each spatch\n",
    "        self.input_size = self.patch_size * self.patch_size * self.n_channels\n",
    "\n",
    "        self.linearProjection = nn.Linear(self.input_size, self.latent_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_data : torch.Tensor, batch_size : int) -> torch.Tensor:\n",
    "        input_data = input_data.to(self.device)\n",
    "        pos_embedding = nn.Parameter(torch.randn(batch_size, 1, self.latent_size)).to(self.device)  # (B, 1, C)\n",
    "\n",
    "        # Patchify\n",
    "        patches = einops.rearrange(\n",
    "            input_data, 'b c (h h1) (w w1) -> b (h w) (h1 w1 c)', h1=self.patch_size, w1=self.patch_size\n",
    "        )\n",
    "\n",
    "        linearProjection = self.linearProjection(patches).to(self.device)\n",
    "        b, n, _ = linearProjection.shape\n",
    "\n",
    "        # Extend pos. embeddings to match number of patches\n",
    "        pos_embed = einops.repeat(pos_embedding, 'b 1 d -> b m d', m=n)\n",
    "        linearProjection += pos_embed\n",
    "        return linearProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Img2SeqModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 device : str,\n",
    "                 patch_size : int,\n",
    "                 dim_model : int,\n",
    "                 encoder_layers : int,\n",
    "                 decoder_layers : int,\n",
    "                 heads : int,\n",
    "                 mlp_dim : int,\n",
    "                 dropout_p: float,\n",
    "                 vocab_size : int,\n",
    "                 n_channels : int,\n",
    "                 max_seq_len : int) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # Init target seq. positional encoding instance\n",
    "        self.pos_enc = PositionalEncoding(\n",
    "            dim_model=dim_model,\n",
    "            dropout_p=dropout_p,\n",
    "            max_len=max_seq_len\n",
    "        )\n",
    "\n",
    "        # Init iamge encoding instance\n",
    "        self.src_embedding = InputEmbedding(patch_size=patch_size,\n",
    "                                            n_channels=n_channels,\n",
    "                                            device=device,\n",
    "                                            latent_size=dim_model)\n",
    "\n",
    "        # Init charachter embeddings table\n",
    "        self.embedding = nn.Embedding(vocab_size, dim_model)\n",
    "\n",
    "        # Instance of encoder layer\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=dim_model,\n",
    "                                                        nhead=heads,\n",
    "                                                        dim_feedforward=mlp_dim,\n",
    "                                                        dropout=dropout_p,\n",
    "                                                        activation='gelu',\n",
    "                                                        batch_first=True)\n",
    "        \n",
    "        # Whole encoder made of encoder layers\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=self.encoder_layer,\n",
    "                                             num_layers=encoder_layers)\n",
    "        \n",
    "        # Instance of decoder layer\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=dim_model,\n",
    "                                                        nhead=heads,\n",
    "                                                        dim_feedforward=mlp_dim,\n",
    "                                                        dropout=dropout_p,\n",
    "                                                        batch_first=True)\n",
    "        \n",
    "        # Whole decoder made of decoder layers\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer=self.decoder_layer,\n",
    "                                             num_layers=decoder_layers)\n",
    "        \n",
    "        # Linear proj. to vocab. size\n",
    "        self.out = nn.Linear(dim_model, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, src : torch.tensor, \n",
    "                tgt : torch.tensor, \n",
    "                tgt_mask : torch.tensor=None, \n",
    "                tgt_pad_mask : torch.tensor=None) -> torch.Tensor:\n",
    "        \n",
    "        src = self.src_embedding(src, batch_size=src.size(0))  # Embed img.\n",
    "        memory = self.encoder(src)  # Encoder output\n",
    "        \n",
    "        # Embed transcriptions\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model) \n",
    "        tgt = self.pos_enc(tgt)\n",
    "\n",
    "        # Out. features of decoder\n",
    "        decoder_out = self.decoder(tgt,\n",
    "                                   memory,\n",
    "                                   tgt_mask=tgt_mask,\n",
    "                                   tgt_key_padding_mask=tgt_pad_mask)\n",
    "        \n",
    "        # Linear projection\n",
    "        out = self.out(decoder_out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def get_tgt_mask(self, size : int) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Creates mask to prevent decoder sequences looking\n",
    "        at future characters.\n",
    "        \"\"\"\n",
    "\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Creates mask that masks padding tokens in decoder. \n",
    "        \"\"\"\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        return (matrix == pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSpectroDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 csv_data : str,\n",
    "                 recordings_folder : str='wav_recordings', \n",
    "                 spectr_folder : str='output_logarithmic',\n",
    "                 spectr_format : str='log',\n",
    "                 produce_spectr : bool=False, \n",
    "                 transform : Any=None):\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv_data)\n",
    "        self.transcriptions, self.wav_filenames = self.df['transcription'].tolist(), self.df['channel_1'].tolist()\n",
    "        self.spectr_folder = spectr_folder\n",
    "        assert len(self.transcriptions) == len(self.wav_filenames)\n",
    "        \n",
    "        # If we want to generate images from WAV-files (not needed if images are already present)\n",
    "        if produce_spectr:\n",
    "            if spectr_format == 'log':\n",
    "                for name in tqdm(self.wav_filenames):\n",
    "                    if os.path.exists(os.path.join(recordings_folder, name)):\n",
    "                        self._logar(os.path.join(recordings_folder, name), spectr_folder)\n",
    "            \n",
    "            elif spectr_format == 'bw':\n",
    "                for name in tqdm(self.wav_filenames):\n",
    "                    if os.path.exists(os.path.join(recordings_folder, name)):\n",
    "                        self._black_n_white(os.path.join(recordings_folder, name), spectr_folder)\n",
    "\n",
    "            elif spectr_format == 'df':\n",
    "                for name in tqdm(self.wav_filenames):\n",
    "                    if os.path.exists(os.path.join(recordings_folder, name)):\n",
    "                        self._default(os.path.join(recordings_folder, name), spectr_folder)\n",
    "\n",
    "\n",
    "        # Acquiring vocab and adding beginning-, end-of-sequence and padding tokens\n",
    "        self.all_text = \"\".join(sample for sample in self.transcriptions)\n",
    "        self.vocab = list(set(self.all_text))\n",
    "        self.vocab.append('<BOS>')\n",
    "        self.vocab.append('<EOS>')\n",
    "        self.vocab.append('<PAD>')\n",
    "\n",
    "        # Mapping vocab to ints and other way around \n",
    "        self.char_to_int = {char:i for i, char in enumerate(self.vocab)}\n",
    "        self.int_to_char = {i:char for char, i in self.char_to_int.items()}\n",
    "\n",
    "    def longest_seq(self) -> int:\n",
    "        # + 2 to count for beginning and end\n",
    "        return max([len(sample['transcription']) for sample in self.data]) + 2\n",
    "    \n",
    "\n",
    "    def get_vocab(self) -> List[str]:\n",
    "        return self.vocab\n",
    "    \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index : int) -> dict[str, torch.tensor]:\n",
    "        img_name, _ = self.wav_filenames[index].split('.')\n",
    "        transcription = self.transcriptions[index]\n",
    "        \n",
    "        if os.path.exists(os.path.join(self.spectr_folder, img_name + '.jpg')):\n",
    "            img = Image.open(os.path.join(self.spectr_folder, fname + '.jpg'))\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "\n",
    "        # Turning str seq into ints, adding start- and end tokens\n",
    "        transformed_seq = [self.char_to_int['<BOS>']]\n",
    "        for ch in seq:\n",
    "            transformed_seq.append(self.char_to_int[ch]) \n",
    "        transformed_seq.append(self.char_to_int['<EOS>'])\n",
    "\n",
    "        return {'img': img, \n",
    "                'seq': torch.tensor(transformed_seq)}\n",
    "        \n",
    "\n",
    "    def _logar(self, wav_file_path : str, jpg_folder_output : str) -> None:\n",
    "        y, sr = librosa.load(wav_file_path)\n",
    "\n",
    "        D = librosa.stft(y)\n",
    "        S = librosa.amplitude_to_db(librosa.magphase(D)[0], ref=np.max)\n",
    "\n",
    "        file_name = os.path.splitext(os.path.basename(wav_file_path))[0]\n",
    "\n",
    "        output_folder = jpg_folder_output\n",
    "\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        output_path = os.path.join(output_folder, f\"{file_name}.jpg\")\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        librosa.display.specshow(S, y_axis='log', x_axis='time')\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.savefig(output_path, bbox_inches = 'tight', pad_inches = 0)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def _black_n_white(self, wav_file_path : str, jpg_folder_output : str) -> None:\n",
    "        y, sr = librosa.load(wav_file_path)\n",
    "\n",
    "        D = librosa.stft(y)\n",
    "        S = librosa.amplitude_to_db(librosa.magphase(D)[0], ref=np.max)\n",
    "\n",
    "        file_name = os.path.splitext(os.path.basename(wav_file_path))[0]\n",
    "\n",
    "        output_folder = jpg_folder_output\n",
    "\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        output_path = os.path.join(output_folder, f\"{file_name}.jpg\")\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        librosa.display.specshow(S, cmap='gray', y_axis='hz', x_axis='time')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(output_path, bbox_inches = 'tight', pad_inches = 0)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def _default(self, wav_file_path : str, jpg_folder_output : str) -> None:\n",
    "        y, sr = librosa.load(wav_file_path)\n",
    "\n",
    "        D = librosa.stft(y)\n",
    "        file_name = os.path.splitext(os.path.basename(wav_file_path))[0]\n",
    "\n",
    "        output_folder = jpg_folder_output\n",
    "\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        output_path = os.path.join(output_folder, f\"{file_name}.jpg\")\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max), y_axis='hz', x_axis='time')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(output_path, bbox_inches = 'tight', pad_inches = 0)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateFunctor:\n",
    "    \"\"\"\n",
    "    Simple collator to pad decoder sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pad_idx : int) -> None:\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    def __call__(self, samples) -> dict[str, torch.tensor]:\n",
    "        img_tensors = []\n",
    "        sequences = []\n",
    "\n",
    "        for sample in sorted(samples, key=lambda x: len(x['seq']), reverse=True):\n",
    "            img_tensors.append(sample['img'])\n",
    "            sequences.append(sample['seq'])\n",
    "        \n",
    "        padded_seq_tensors = pad_sequence(sequences, \n",
    "                                          batch_first=True,\n",
    "                                          padding_value=self.pad_idx)\n",
    "        \n",
    "        img_tensors = torch.stack(tuple(img_tensors), dim=0)\n",
    "        return {'img': img_tensors,\n",
    "                'seq': padded_seq_tensors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset variables \n",
    "data_path = 'dataset.csv' \n",
    "produce_spectr = False\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((620, 620)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = CustomSpectroDataset(csv_data=data_path,\n",
    "                               produce_spectr=produce_spectr,\n",
    "                               transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomSpectroDataset' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\experimental_upd.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m seed \u001b[39m=\u001b[39m \u001b[39m7\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m gen \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mGenerator()\u001b[39m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_dataset, test_dataset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mrandom_split(dataset, [\u001b[39m0.9\u001b[39;49m, \u001b[39m0.1\u001b[39;49m], generator\u001b[39m=\u001b[39;49mgen)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(train_dataset, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                               batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                               shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                               collate_fn\u001b[39m=\u001b[39mCollateFunctor(pad_idx\u001b[39m=\u001b[39mdataset\u001b[39m.\u001b[39mchar_to_int[\u001b[39m'\u001b[39m\u001b[39m<PAD>\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m test_dataloader \u001b[39m=\u001b[39m DataLoader(test_dataset, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                              batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                              shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                              collate_fn\u001b[39m=\u001b[39mCollateFunctor(pad_idx\u001b[39m=\u001b[39mdataset\u001b[39m.\u001b[39mchar_to_int[\u001b[39m'\u001b[39m\u001b[39m<PAD>\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:403\u001b[0m, in \u001b[0;36mrandom_split\u001b[1;34m(dataset, lengths, generator)\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[39mif\u001b[39;00m frac \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m frac \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFraction at index \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m is not between 0 and 1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    402\u001b[0m     n_items_in_split \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\n\u001b[1;32m--> 403\u001b[0m         math\u001b[39m.\u001b[39mfloor(\u001b[39mlen\u001b[39;49m(dataset) \u001b[39m*\u001b[39m frac)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     )\n\u001b[0;32m    405\u001b[0m     subset_lengths\u001b[39m.\u001b[39mappend(n_items_in_split)\n\u001b[0;32m    406\u001b[0m remainder \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataset) \u001b[39m-\u001b[39m \u001b[39msum\u001b[39m(subset_lengths)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\experimental_upd.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X10sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CustomSpectroDataset' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "# Train/test splits\n",
    "seed = 7\n",
    "gen = torch.Generator().manual_seed(seed)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.9, 0.1], generator=gen)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=32, \n",
    "                              shuffle=True,\n",
    "                              collate_fn=CollateFunctor(pad_idx=dataset.char_to_int['<PAD>']))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                             batch_size=32, \n",
    "                             shuffle=True,\n",
    "                             collate_fn=CollateFunctor(pad_idx=dataset.char_to_int['<PAD>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Hyperparameters\n",
    "patch_size = 20\n",
    "dim_model = 528\n",
    "encoder_layers = 6\n",
    "decoder_layers = 6\n",
    "heads = 6\n",
    "mlp_dim = 2048\n",
    "dropout_p = 0.1\n",
    "\n",
    "vocab_size = len(dataset.get_vocab())\n",
    "max_seq_len = dataset.longest_seq()\n",
    "n_channels = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Img2SeqModel(device=device,\n",
    "                     patch_size=patch_size,\n",
    "                     dim_model=dim_model,\n",
    "                     encoder_layers=encoder_layers,\n",
    "                     decoder_layers=decoder_layers,\n",
    "                     heads=heads,\n",
    "                     mlp_dim=mlp_dim,\n",
    "                     dropout_p=dropout_p,\n",
    "                     vocab_size=vocab_size,\n",
    "                     n_channels=n_channels,\n",
    "                     max_seq_len=max_seq_len)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "init_lr = 1e-4\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=init_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(iterator):\n",
    "        img = batch['img'].to(device)\n",
    "        target_seq = batch['seq'].to(device)\n",
    "\n",
    "        sequence_len = target_seq.size(1)\n",
    "        tgt_attention_mask = model.get_tgt_mask(sequence_len).to(device)\n",
    "        tgt_pad_mask = model.create_pad_mask(target_seq, dataset.char_to_int['<PAD>'])\n",
    "\n",
    "        logits = model(img, target_seq, tgt_mask=tgt_attention_mask, tgt_pad_mask=tgt_pad_mask)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        target_seq = target_seq.view(B*T)\n",
    "        loss = F.cross_entropy(logits, target_seq, ignore_index=dataset.char_to_int['<PAD>'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "    \n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/263 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Operation on closed image",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\experimental_upd.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     epoch_loss \u001b[39m=\u001b[39m train(model, train_dataloader, optimizer, device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{\u001b[39;00mepoch_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)    \n",
      "\u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\experimental_upd.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m batch \u001b[39min\u001b[39;49;00m tqdm(iterator):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     img \u001b[39m=\u001b[39;49m batch[\u001b[39m'\u001b[39;49m\u001b[39mimg\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     target_seq \u001b[39m=\u001b[39;49m batch[\u001b[39m'\u001b[39;49m\u001b[39mseq\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device)\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\tqdm\\std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;49;00m obj \u001b[39min\u001b[39;49;00m iterable:\n\u001b[0;32m   1183\u001b[0m         \u001b[39myield\u001b[39;49;00m obj\n\u001b[0;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m\"\u001b[39m\u001b[39m__getitems__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
      "\u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\experimental_upd.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X15sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m img, seq \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index][\u001b[39m'\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index][\u001b[39m'\u001b[39m\u001b[39mtranscription\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X15sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X15sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X15sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# Turning str seq into ints, adding start- and end tokens\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/martook/Desktop/projects/blatj/experimental_upd.ipynb#X15sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m transformed_seq \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchar_to_int[\u001b[39m'\u001b[39m\u001b[39m<BOS>\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m    354\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\torchvision\\transforms\\functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    488\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    489\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 490\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49moutput_size, interpolation\u001b[39m=\u001b[39;49mpil_interpolation)\n\u001b[0;32m    492\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39moutput_size, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue, antialias\u001b[39m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(size, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot inappropriate size arg: \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mresize(\u001b[39mtuple\u001b[39;49m(size[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]), interpolation)\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\PIL\\Image.py:2163\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   2161\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(size)\n\u001b[1;32m-> 2163\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   2164\u001b[0m \u001b[39mif\u001b[39;00m box \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2165\u001b[0m     box \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\PIL\\ImageFile.py:162\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcannot load this image\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[1;32m--> 162\u001b[0m pixel \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mImage\u001b[39m.\u001b[39;49mload(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtile:\n\u001b[0;32m    164\u001b[0m     \u001b[39mreturn\u001b[39;00m pixel\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\PIL\\Image.py:855\u001b[0m, in \u001b[0;36mImage.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpyaccess:\n\u001b[0;32m    854\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpyaccess\n\u001b[1;32m--> 855\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim\u001b[39m.\u001b[39;49mpixel_access(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadonly)\n",
      "File \u001b[1;32mc:\\Users\\martook\\Desktop\\projects\\blatj\\spectrorgram_project\\Lib\\site-packages\\PIL\\_util.py:19\u001b[0m, in \u001b[0;36mDeferredError.__getattr__\u001b[1;34m(self, elt)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, elt):\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mex\n",
      "\u001b[1;31mValueError\u001b[0m: Operation on closed image"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {epoch + 1}', end='\\n\\n')\n",
    "    epoch_loss = train(model, train_dataloader, optimizer, device)\n",
    "    print()\n",
    "    print(f'Loss: {epoch_loss}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4]], device='cuda:0')\n",
      "tensor([[1, 4, 4]], device='cuda:0')\n",
      "tensor([[1, 4, 4, 4]], device='cuda:0')\n",
      "tensor([[1, 4, 4, 4, 4]], device='cuda:0')\n",
      "tensor([[1, 4, 4, 4, 4, 4]], device='cuda:0')\n",
      "tensor([[1, 4, 4, 4, 4, 4, 4]], device='cuda:0')\n",
      "tensor([[1, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0')\n",
      "tensor([[1, 4, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0')\n",
      "tensor([[1, 4, 4, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0')\n",
      "tensor([[1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0')\n",
      "tensor([[1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0')\n",
      "tensor([[1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0')\n",
      "tensor([[1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0')\n",
      "tensor([[1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0')\n",
      "tensor([[1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, img, device):\n",
    "    model.eval()\n",
    "    seq_inp = torch.tensor([1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    for _ in range(15):\n",
    "        logits = model(img, seq_inp)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1).detach().to('cpu')\n",
    "        idx_next = np.argmax(probs, axis=-1).unsqueeze(0).to(device)\n",
    "        seq_inp = torch.cat(((seq_inp, idx_next)), dim=1)\n",
    "        print(seq_inp)\n",
    "batch = next(iter(test_dataloader))\n",
    "img = batch['img'][0].unsqueeze(0)\n",
    "generate(model, img, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try:\n",
    "- Resolve data problems to increase training samples\n",
    "- Try lowering lr\n",
    "- Try lowering number of parameters\n",
    "- Try different sampling stratetgies from the decoder\n",
    "- Potentially pre-trained decoder?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spectrorgram_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
