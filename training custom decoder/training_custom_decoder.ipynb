{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes positional encoding as given in paper 'Attention is all you need'.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model : int, dropout_p : float, max_len : int) -> torch.Tensor:\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:, :token_embedding.size(1), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NorwegianDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset to process .xml-files containing\n",
    "    texts from Norwegian Newspaper corpus. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        # Utilizing NorBERT tokenizer, adding special tokens     \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('ltg/norbert2')\n",
    "        new_special_tokens = {'additional_special_tokens' : ['[BOS]', '[EOS]']}\n",
    "        self.tokenizer.add_special_tokens(new_special_tokens)\n",
    "\n",
    "        files  = os.listdir('docs')  # Here is where .xml files of Norwegian corpus are supposed to be \n",
    "        self.texts = []\n",
    "\n",
    "        # Processing files\n",
    "        for file in tqdm(files):\n",
    "            with open(os.path.join('docs', file), 'r', encoding='utf8') as f:\n",
    "                data = f.read()\n",
    "                bs_data = BeautifulSoup(data, \"xml\")\n",
    "                text = bs_data.find('div', {'type':'text'})\n",
    "                if text:\n",
    "                    self.texts.append(text.get_text())\n",
    "\n",
    "        self.sentences = []\n",
    "        self.longest_sequence = 0\n",
    "\n",
    "        # Extracting sequences + longest seq. len\n",
    "        for text in tqdm(self.texts):\n",
    "            text = text.split('\\n')\n",
    "            sentences_list = text[1:]\n",
    "            for sent in sentences_list:\n",
    "                tokenized_len = len(self.tokenizer.tokenize(sent))\n",
    "                if tokenized_len > self.longest_sequence:\n",
    "                    self.longest_sequence = tokenized_len\n",
    "            self.sentences.extend(sentences_list)\n",
    "           \n",
    "\n",
    "    def __getitem__(self, index : int) -> dict[str, torch.tensor]:\n",
    "        text =self.sentences[index]\n",
    "\n",
    "        seq_to_int = [self.tokenizer.vocab['[BOS]']]\n",
    "        seq_to_int.extend(self.tokenizer.encode(text, add_special_tokens=False))\n",
    "        seq_to_int.append(self.tokenizer.vocab['[EOS]'])\n",
    "\n",
    "        # Remember offset of 1: we are trying to predict future tokens\n",
    "        src = seq_to_int[:-1]\n",
    "        tgt = seq_to_int[1:]\n",
    "        \n",
    "        return {'src': torch.tensor(src),\n",
    "                'tgt': torch.tensor(tgt)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    \n",
    "    def longest_sequence(self):\n",
    "        return self.longest_sequence + 1\n",
    "    \n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NorwegianCollator:\n",
    "    \"\"\"\n",
    "    Simple collator to pad decoder sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pad_idx : int) -> None:\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    def __call__(self, samples) -> dict[str, torch.tensor]: \n",
    "        srcs = []\n",
    "        tgts = []\n",
    "\n",
    "        for sample in sorted(samples, key=lambda x: len(x['src']), reverse=True):\n",
    "            srcs.append(sample['src'])\n",
    "            tgts.append(sample['tgt'])\n",
    "\n",
    "        # Padding sequences\n",
    "        padded_src_tensors = pad_sequence(srcs, \n",
    "                                          batch_first=True,\n",
    "                                          padding_value=self.pad_idx)\n",
    "        \n",
    "        padded_tgt_tensors = pad_sequence(tgts, \n",
    "                                          batch_first=True,\n",
    "                                          padding_value=self.pad_idx)\n",
    "\n",
    "        \n",
    "        return {'src': padded_src_tensors,\n",
    "                'tgt': padded_tgt_tensors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NorwegianModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Transformer-based encoder-decoder modek;\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim_model : int,\n",
    "                 decoder_layers : int,\n",
    "                 heads : int,\n",
    "                 mlp_dim : int,\n",
    "                 dropout_p: float,\n",
    "                 max_seq_len : int,\n",
    "                 vocab_size : int) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # Init target seq. positional encoding instance\n",
    "        self.pos_enc = PositionalEncoding(\n",
    "            dim_model=dim_model,\n",
    "            dropout_p=dropout_p,\n",
    "            max_len=max_seq_len\n",
    "        )\n",
    "\n",
    "        # Init charachter embeddings table\n",
    "        self.embedding = nn.Embedding(vocab_size, dim_model)\n",
    "    \n",
    "        # Instance of decoder layer\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=dim_model,\n",
    "                                                   dim_feedforward=mlp_dim,\n",
    "                                                   nhead=heads,\n",
    "                                                   dropout=dropout_p,\n",
    "                                                   batch_first=True)\n",
    "        \n",
    "        # Whole decoder made of decoder layers\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer,\n",
    "                                             num_layers=decoder_layers)\n",
    "\n",
    "        # Linear proj. to vocab. size\n",
    "        self.out = nn.Linear(dim_model, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, src : torch.tensor, \n",
    "                      tgt : torch.tensor,\n",
    "                      src_pad_mask=None, \n",
    "                      tgt_mask : torch.tensor=None, \n",
    "                      tgt_pad_mask : torch.tensor=None) -> torch.Tensor:\n",
    "            \n",
    "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
    "        src = self.pos_enc(src)\n",
    "        tgt = self.pos_enc(tgt)\n",
    "\n",
    "        decoder_out = self.decoder(src, tgt, tgt_mask=tgt_mask, memory_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "        out = self.out(decoder_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "\n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        return (matrix == pad_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norwegian_dataset = NorwegianDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "dim_model = 768\n",
    "decoder_layers = 6\n",
    "heads = 12\n",
    "mlp_dim = 2048\n",
    "dropout_p = 0.1\n",
    "\n",
    "vocab_size = norwegian_dataset.vocab_size()\n",
    "max_seq_len = norwegian_dataset.longest_sequence + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norwegian_model = NorwegianModel(dim_model=dim_model,\n",
    "                                 decoder_layers=decoder_layers,\n",
    "                                 heads=heads,\n",
    "                                 mlp_dim=mlp_dim,\n",
    "                                 dropout_p=dropout_p,\n",
    "                                 max_seq_len=max_seq_len,\n",
    "                                 vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparemeters \n",
    "epochs = 2\n",
    "init_lr = 1e-3\n",
    "optimizer = torch.optim.AdamW(norwegian_model.parameters(), lr=init_lr)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norwegian_model = norwegian_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, device):\n",
    "    \"\"\"\n",
    "    Generates text based on BOS-token + potentially some other token of choice.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    src = torch.tensor([norwegian_dataset.tokenizer.vocab['[BOS]'],\n",
    "                        norwegian_dataset.tokenizer.vocab['fra' ]], dtype=torch.int64, device=device).unsqueeze(0)\n",
    "    \n",
    "    tgt = torch.tensor([norwegian_dataset.tokenizer.vocab['deg']], dtype=torch.int64, device=device).unsqueeze(0)\n",
    "\n",
    "    for _ in range(10):\n",
    "        logits = model(src, tgt)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1).detach().to('cpu')\n",
    "        idx_next = np.argmax(probs, axis=-1).unsqueeze(0).to(device)\n",
    "        tgt = torch.cat(((tgt, idx_next)), dim=1)\n",
    "    \n",
    "    print(norwegian_dataset.tokenizer.decode(tgt.squeeze(0).detach().to('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train loop\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    eval_batches = [10000, 20000, 30000, 40000, 50000]\n",
    "    for batch_num, batch in enumerate(tqdm(iterator), 1):\n",
    "        src = batch['src'].to(device)\n",
    "        tgt = batch['tgt'].to(device)\n",
    "\n",
    "        tgt_len = tgt.size(1)\n",
    "\n",
    "        # Attention mask for decoder + pad masks for encoder and decoder\n",
    "        tgt_attention_mask = model.get_tgt_mask(tgt_len).to(device)\n",
    "        src_pad_mask = model.create_pad_mask(src, norwegian_dataset.tokenizer.pad_token_id)\n",
    "        tgt_pad_mask = model.create_pad_mask(tgt, norwegian_dataset.tokenizer.pad_token_id)\n",
    "\n",
    "        logits = model(src, tgt, tgt_mask=tgt_attention_mask, tgt_pad_mask=tgt_pad_mask, src_pad_mask=src_pad_mask)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        tgt = tgt.view(B*T)\n",
    "        loss = F.cross_entropy(logits, tgt, ignore_index=norwegian_dataset.tokenizer.pad_token_id)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "        if batch_num in eval_batches:\n",
    "            generate(model, device)\n",
    "             \n",
    "\n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(norwegian_dataset, \n",
    "                              batch_size=16, \n",
    "                              shuffle=True,\n",
    "                              collate_fn=NorwegianCollator(pad_idx=norwegian_dataset.tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {epoch + 1}', end='\\n\\n')\n",
    "    epoch_loss = train(norwegian_model, train_dataloader, optimizer, device)\n",
    "    print()\n",
    "    print(f'Loss: {epoch_loss}')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
