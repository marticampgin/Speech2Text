{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9311765-e0ea-4fc9-8303-86194d7e084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os \n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import timm\n",
    "import wave\n",
    "import skimage.io\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "from torch.nn import functional as F\n",
    "from typing import List, Any, Tuple, Optional, TypeVar, Union, IO, Type\n",
    "from transformers import AutoTokenizer\n",
    "from timm.models.layers import to_2tuple\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "plt.ioff()\n",
    "matplotlib.use('agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7747aa47-25bc-4b00-a3dc-f3d11a9411d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bahdanau Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        self.W = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.U = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.A = nn.Linear(attention_dim, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, features, hidden_state):\n",
    "        u_hs = self.U(features)      # (batch_size, num_layers, attention_dim)\n",
    "        w_ah = self.W(hidden_state)  # (batch_size, attention_dim)\n",
    "        \n",
    "        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1))  # (batch_size, num_layers, attemtion_dim)\n",
    "        \n",
    "        attention_scores = self.A(combined_states)         # (batch_size, num_layers, 1)\n",
    "        attention_scores = attention_scores.squeeze(2)     # (batch_size, num_layers)\n",
    "        \n",
    "        \n",
    "        alpha = F.softmax(attention_scores,dim=1)          # (batch_size, num_layers)\n",
    "        \n",
    "        attention_weights = features * alpha.unsqueeze(2)  # (batch_size, num_layers, features_dim)\n",
    "        attention_weights = attention_weights.sum(dim=1)   # (batch_size, num_layers)\n",
    "        \n",
    "        return alpha, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(True)\n",
    "        \n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)                                     #  (batch_size, 2048, 7, 7)\n",
    "        features = features.permute(0, 2, 3, 1)                            #  (batch_size, 7, 7, 2048)\n",
    "        features = features.view(features.size(0), -1, features.size(-1))  #  (batch_size, 49, 2048)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aa25614-8b68-4bf9-85c1-324fcee0002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,embed_size, \n",
    "                 vocab_size,\n",
    "                 attention_dim,\n",
    "                 encoder_dim,\n",
    "                 decoder_dim,\n",
    "                 drop_prob=0.3,\n",
    "                 device='cuda'):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.attention_dim = attention_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
    "        \n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.lstm_cell = nn.LSTMCell(embed_size + encoder_dim, decoder_dim, bias=True)\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
    "        \n",
    "        self.fcn = nn.Linear(decoder_dim, vocab_size)\n",
    "        self.drop = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        # Vectorize the caption\n",
    "        embeds = self.embedding(captions)\n",
    "        \n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
    "        \n",
    "        # Get the seq length to iterate\n",
    "        seq_length = len(captions[0]) -1 # Exclude the last one\n",
    "        batch_size = captions.size(0)\n",
    "        num_features = features.size(1)\n",
    "        \n",
    "        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(self.device)\n",
    "        alphas = torch.zeros(batch_size, seq_length,num_features).to(self.device)\n",
    "                \n",
    "        for s in range(seq_length):\n",
    "            alpha, context = self.attention(features, h)\n",
    "            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "                    \n",
    "            output = self.fcn(self.drop(h))\n",
    "            \n",
    "            preds[:,s] = output\n",
    "            alphas[:,s] = alpha  \n",
    "        \n",
    "        return preds, alphas\n",
    "    \n",
    "    \n",
    "    def generate_caption(self, features, int2str, str2int, max_len=20):\n",
    "        # Given the image features generate the captions\n",
    "        batch_size = features.size(0)\n",
    "        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
    "        \n",
    "        alphas = []\n",
    "        \n",
    "        # Starting input\n",
    "        word = torch.tensor(str2int['[BOS]']).view(1,-1).to(self.device)\n",
    "        embeds = self.embedding(word)\n",
    "\n",
    "        captions = []\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            alpha, context = self.attention(features, h)\n",
    "            \n",
    "            # Store the alpha score\n",
    "            alphas.append(alpha.cpu().detach().numpy())\n",
    "            \n",
    "            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "            output = self.fcn(self.drop(h))\n",
    "            output = output.view(batch_size,-1)\n",
    "        \n",
    "            # Select the word with most val\n",
    "            predicted_word_idx = output.argmax(dim=1)\n",
    "            \n",
    "            # Save the generated word\n",
    "            captions.append(predicted_word_idx.item())\n",
    "            \n",
    "            # End if \"[EOS]\" detected\n",
    "            if int2str[predicted_word_idx.item()] == \"[EOS]\":\n",
    "                break\n",
    "            \n",
    "            # Send generated word as the next caption\n",
    "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
    "        \n",
    "        # Covert the vocab idx to words and return sentence\n",
    "        return [int2str[caption] for caption in captions], alphas\n",
    "    \n",
    "    \n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c0ae951-ab2d-487f-ba5f-315817c5501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Img2SeqModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 encoder_dim,\n",
    "                 decoder_dim,\n",
    "                 attention_dim,\n",
    "                 vocab_size) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # ------------ INITIALIZING ENCODER ------------\n",
    "\n",
    "        self.encoder = EncoderCNN()\n",
    "\n",
    "        # ------------ INITIALIZING DECODER ------------\n",
    "        self.decoder = DecoderRNN(embed_size=embed_dim,\n",
    "                                  vocab_size=vocab_size,\n",
    "                                  attention_dim=attention_dim,\n",
    "                                  encoder_dim=encoder_dim,\n",
    "                                  decoder_dim=decoder_dim)\n",
    "\n",
    "\n",
    "    def forward(self, src : torch.tensor, \n",
    "                tgt : torch.tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Encode\n",
    "        features = self.encoder(src)\n",
    "        # Decode\n",
    "        outputs = self.decoder(features, tgt)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def get_shape(self, fstride, tstride, height, width):\n",
    "        test_input = torch.randn(1, 1, height, width)\n",
    "        test_proj = nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
    "        test_out = test_proj(test_input)\n",
    "        f_dim = test_out.shape[2]\n",
    "        t_dim = test_out.shape[3]\n",
    "        return f_dim, t_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c16e9-5b82-42af-af40-d918750a40c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSpectroDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 csv_data : str,\n",
    "                 recordings_folder : str='wav_recordings', \n",
    "                 spectr_folder : str='log_rectangular_small',\n",
    "                 produce_spectr : bool=False, \n",
    "                 transform : Any=None,\n",
    "                 ):\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv_data)\n",
    "        self.df.dropna(how='any', inplace=True)\n",
    "        self.spectr_folder = spectr_folder\n",
    "        self.jpg_paths = os.listdir(self.spectr_folder)\n",
    "        self.wav_filenames = self.df['channel_1'].tolist()\n",
    "        self.transcriptions = self.df['transcription'].tolist()\n",
    "        assert len(self.wav_filenames) == len(self.transcriptions)\n",
    "\n",
    "        # If we want to generate images from WAV-files (not needed if images are already present)\n",
    "        if produce_spectr:\n",
    "            for name in tqdm(self.wav_filenames):\n",
    "                if os.path.exists(os.path.join(recordings_folder, name)):\n",
    "                    self._logar(os.path.join(recordings_folder, name), spectr_folder)\n",
    "            \n",
    "        self.data = []\n",
    "\n",
    "        for i, wav in enumerate(tqdm(self.wav_filenames)):\n",
    "            for jpg in self.jpg_paths:\n",
    "                if jpg.split('.')[0] == wav.split('.')[0]:\n",
    "                    self.data.append({'img_name': jpg, 'seq': self.transcriptions[i], 'wav': wav})\n",
    "        \n",
    "        # SUPER SIMPLE TOKENIZER: SPLIT ON SPACEBAR\n",
    "        self.int2str = {0: '[PAD]', 1: '[BOS]', 2: '[EOS]'}\n",
    "        \n",
    "        all_tokens = []\n",
    "\n",
    "        for sample in self.data:\n",
    "            transcription = sample['seq']\n",
    "            tokens = transcription.split()\n",
    "            all_tokens.extend(tokens)\n",
    "\n",
    "        vocab = list(set(all_tokens))\n",
    "\n",
    "        for token in vocab:\n",
    "            self.int2str[len(self.int2str)] = token\n",
    "        \n",
    "        self.str2int = {st:i for i, st in self.int2str.items()}\n",
    "\n",
    "    def __getitem__(self, index : int) -> dict[str, torch.tensor]:\n",
    "        img_name = self.data[index]['img_name']\n",
    "        seq = self.data[index]['seq']\n",
    "        wav = self.data[index]['wav']\n",
    "\n",
    "        with Image.open(os.path.join(self.spectr_folder, img_name)) as img_file:\n",
    "            if self.transform:\n",
    "                img_tensor = self.transform(img_file)\n",
    "\n",
    "        # seq_to_int = [self.tokenizer.vocab['[BOS]']]\n",
    "        seq_to_int = [self.str2int['[BOS]']]\n",
    "        seq_to_int.extend([self.str2int[token] for token in seq.split()])\n",
    "        seq_to_int.append(self.str2int['[EOS]'])\n",
    "\n",
    "        return {'img': img_tensor, \n",
    "                'seq': torch.tensor(seq_to_int),\n",
    "                'wav': wav}\n",
    "\n",
    "    def get_vocab_size(self) -> List[str]:\n",
    "        #return len(self.tokenizer.vocab)\n",
    "        return len(self.int2str)\n",
    "    \n",
    "    \n",
    "    def get_all_transcriptions(self) -> List[dict[torch.tensor, str]]:\n",
    "        transcriptions = [sample['seq'] for sample in self.data]\n",
    "        return transcriptions\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "      \n",
    "\n",
    "    def _logar(self, wav_file_path : str, jpg_folder_output : str) -> None:\n",
    "        y, sr = librosa.load(wav_file_path)\n",
    "        \n",
    "        D = librosa.stft(y)\n",
    "        S = librosa.amplitude_to_db(librosa.magphase(D)[0], ref=np.max)  # different sizes here\n",
    "        file_name = os.path.splitext(os.path.basename(wav_file_path))[0]\n",
    "\n",
    "        output_folder = jpg_folder_output\n",
    "\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        output_path = os.path.join(output_folder, f\"{file_name}.jpg\")\n",
    "\n",
    "        plt.figure(figsize=(7, 3))\n",
    "\n",
    "        librosa.display.specshow(S, y_axis='log', x_axis='time')\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.savefig(output_path, bbox_inches = 'tight', pad_inches = 0)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "886ffac9-0e4a-4e3a-b605-4775dc154617",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateFunctor:\n",
    "    \"\"\"\n",
    "    Simple collator to pad decoder sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pad_idx : int) -> None:\n",
    "        self.pad_idx = pad_idx\n",
    "        self.max = 1024\n",
    "        \n",
    "    def __call__(self, samples) -> dict[str, torch.tensor]:\n",
    "        img_tensors = []\n",
    "        sequences = []\n",
    "        wavs = []\n",
    "\n",
    "        for sample in sorted(samples, key=lambda x: len(x['seq']), reverse=True):\n",
    "            img_tensors.append(sample['img'])\n",
    "            sequences.append(sample['seq'])\n",
    "            wavs.append(sample['wav'])\n",
    "\n",
    "        # Padding sequences\n",
    "        padded_seq_tensors = pad_sequence(sequences, \n",
    "                                          batch_first=True,\n",
    "                                          padding_value=self.pad_idx)\n",
    "        \n",
    "        # Padding images\n",
    "        for i, tensor in enumerate(img_tensors):\n",
    "            img_tensors[i] = F.pad(tensor, (0, self.max - tensor.shape[-1]), 'constant', 0)\n",
    "        \n",
    "        img_tensors = torch.stack(tuple(img_tensors), dim=0)\n",
    "        return {'img': img_tensors,\n",
    "                'seq': padded_seq_tensors,\n",
    "                'wav': wavs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cfbd79-549f-4914-862f-ae0cd7f5ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset variables\n",
    "data_path = 'dataset.csv' \n",
    "produce_spectr = False\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = CustomSpectroDataset(csv_data=data_path,\n",
    "                               produce_spectr=produce_spectr,\n",
    "                               transform=transform,\n",
    "                               spectr_folder='log_rectangular_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d58fc28b-926c-4a5b-8765-adccdf6ec717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test splits\n",
    "seed = 77\n",
    "gen = torch.Generator().manual_seed(seed)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.9, 0.1], generator=gen)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=16, \n",
    "                              shuffle=True, \n",
    "                              collate_fn=CollateFunctor(pad_idx=dataset.str2int['[PAD]']))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                             batch_size=16, \n",
    "                             shuffle=True,\n",
    "                             collate_fn=CollateFunctor(pad_idx=dataset.str2int['[PAD]']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6e8d978-de8d-4145-9e21-d3860fc003b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Hyperparameters\n",
    "dim_model = 768\n",
    "dropout_p = 0.1\n",
    "vocab_size = dataset.get_vocab_size()\n",
    "img_shape_small = (1, 128, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1574a4d-3951-4d16-9674-dfa39157cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Img2SeqModel(embed_dim=300,\n",
    "                     encoder_dim=2048,\n",
    "                     decoder_dim=512,\n",
    "                     attention_dim=256,\n",
    "                     vocab_size=vocab_size)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de555963-bf03-4cb4-976b-24a702c9b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "enc_lr = 1e-4\n",
    "dec_lr = 3e-4\n",
    "enc_optim = torch.optim.AdamW(model.encoder.parameters(), lr=enc_lr)\n",
    "dec_optim = torch.optim.AdamW(model.decoder.parameters(), lr=dec_lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.str2int['[PAD]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91e3d634-8c5a-45f2-8d4a-33e960e84781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, enc_optim, dec_optim, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(iterator):\n",
    "        img = batch['img'].to(device)\n",
    "        target_seq = batch['seq'].to(device)\n",
    "\n",
    "        outputs, _ = model(img, target_seq)\n",
    "        targets = target_seq[:, 1:]\n",
    "        \n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        enc_optim.step()\n",
    "        dec_optim.step()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "  \n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e51c94e-bc83-4a3a-bfbd-e387fb80ee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = 10\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, iterator, device):\n",
    "    model.eval()\n",
    "    batch = next(iter(iterator))\n",
    "    \n",
    "    # Evaluatuing on 10 spectrograms\n",
    "    for i in range(evaluations):\n",
    "        img = batch['img'][i].unsqueeze(0).to(device)\n",
    "        target_seq = batch['seq'][i]\n",
    "\n",
    "        features = model.encoder(img)\n",
    "        caps, _ = model.decoder.generate_caption(features, int2str=dataset.int2str, str2int=dataset.str2int, max_len=16)\n",
    "        caption = ' '.join(caps)\n",
    "        target = [dataset.int2str[i.item()] for i in target_seq.detach().cpu()]\n",
    "\n",
    "        print(f'Generated caption: {caption}')\n",
    "        print(f'Target caption: {\" \".join(t for t in target)}', end='\\n-----------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1ac1a-f029-486b-b278-c5b449d93976",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch_loss = train(model, train_dataloader, enc_optim, dec_optim, device)\n",
    "    print(f'Epoch: {epoch + 1}, Loss: {epoch_loss}', end='\\n\\n')\n",
    "    evaluate(model, test_dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
