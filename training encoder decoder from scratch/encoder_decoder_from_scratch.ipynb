{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os \n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import math\n",
    "import einops\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Any, Union, Tuple\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "plt.ioff()\n",
    "matplotlib.use('agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes positional encoding as given in paper 'Attention is all you need'.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model : int, dropout_p : float, max_len : int) -> torch.Tensor:\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:, :token_embedding.size(1), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddingRecPatches(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes embeddings for each patch in the image.\n",
    "    Can use rectangular patches.  \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 patch_size : int,   \n",
    "                 latent_size : int,\n",
    "                 img_shape: Tuple[int],\n",
    "                 device : str) -> torch.Tensor:\n",
    "\n",
    "        super(InputEmbeddingRecPatches, self).__init__()\n",
    "\n",
    "        c, h, w = img_shape\n",
    "        self.patch_size = (patch_size, patch_size) if isinstance(patch_size, int) else patch_size\n",
    "        assert h % patch_size[0] == 0 and w % patch_size[1] == 0,\\\n",
    "            'Image dim. must be divisible by the patch size'\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        self.input_size = self.patch_size[0] * self.patch_size[1] * c  # Embedding size of each patch\n",
    "        self.linearProjection = nn.Linear(self.input_size, self.latent_size)\n",
    "\n",
    "    def forward(self, input_data : torch.Tensor, batch_size : int) -> torch.Tensor:\n",
    "        input_data = input_data.to(self.device)\n",
    "        pos_embedding = nn.Parameter(torch.randn(batch_size, 1, self.latent_size)).to(self.device)  # (B, 1, C)\n",
    "        h1, w1 = self.patch_size[0], self.patch_size[1]\n",
    "        # Patchify\n",
    "        patches = einops.rearrange(\n",
    "            input_data, 'b c (h h1) (w w1) -> b (h w) (h1 w1 c)', h1=h1, w1=w1\n",
    "        )\n",
    "\n",
    "        linearProjection = self.linearProjection(patches).to(self.device)\n",
    "        b, n, _ = linearProjection.shape\n",
    "\n",
    "        # Extend pos. embeddings to match number of patches\n",
    "        pos_embed = einops.repeat(pos_embedding, 'b 1 d -> b m d', m=n)\n",
    "        linearProjection += pos_embed\n",
    "        return linearProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Img2SeqModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 device : str,\n",
    "                 patch_size : Union[int, tuple[int]],\n",
    "                 dim_model : int,\n",
    "                 encoder_layers : int,\n",
    "                 decoder_layers : int,\n",
    "                 heads : int,\n",
    "                 mlp_dim : int,\n",
    "                 dropout_p: float,\n",
    "                 vocab_size : int,\n",
    "                 max_seq_len : int,\n",
    "                 img_shape : Tuple[int]) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # Init target seq. positional encoding instance\n",
    "        self.pos_enc = PositionalEncoding(\n",
    "            dim_model=dim_model,\n",
    "            dropout_p=dropout_p,\n",
    "            max_len=max_seq_len\n",
    "        )\n",
    "\n",
    "        # Init iamge encoding instance\n",
    "        self.src_embedding = InputEmbeddingRecPatches(\n",
    "            patch_size=patch_size,\n",
    "            latent_size=dim_model,\n",
    "            img_shape=img_shape,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Init token embeddings table\n",
    "        self.embedding = nn.Embedding(vocab_size, dim_model)\n",
    "\n",
    "        # Instance of encoder layer\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=dim_model,\n",
    "                                                        nhead=heads,\n",
    "                                                        dim_feedforward=mlp_dim,\n",
    "                                                        dropout=dropout_p,\n",
    "                                                        activation='gelu',\n",
    "                                                        batch_first=True)\n",
    "        \n",
    "        # Whole encoder made of encoder layers\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=self.encoder_layer,\n",
    "                                             num_layers=encoder_layers)\n",
    "        \n",
    "        # Instance of decoder layer\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=dim_model,\n",
    "                                                        nhead=heads,\n",
    "                                                        dim_feedforward=mlp_dim,\n",
    "                                                        dropout=dropout_p,\n",
    "                                                        batch_first=True)\n",
    "        \n",
    "        # Whole decoder made of decoder layers\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer=self.decoder_layer,\n",
    "                                             num_layers=decoder_layers)\n",
    "        \n",
    "        # Linear proj. to vocab. size\n",
    "        self.out = nn.Linear(dim_model, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, src : torch.tensor, \n",
    "                tgt : torch.tensor, \n",
    "                tgt_mask : torch.tensor=None, \n",
    "                tgt_pad_mask : torch.tensor=None) -> torch.Tensor:\n",
    "        \n",
    "        src = self.src_embedding(src, batch_size=src.size(0))  # Embed img.\n",
    "        memory = self.encoder(src)  # Encoder output\n",
    "        \n",
    "        # Embed transcriptions\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model) \n",
    "        tgt = self.pos_enc(tgt)\n",
    "\n",
    "        # Out. features of decoder\n",
    "        decoder_out = self.decoder(tgt,\n",
    "                                   memory,\n",
    "                                   tgt_mask=tgt_mask,\n",
    "                                   tgt_key_padding_mask=tgt_pad_mask)\n",
    "        \n",
    "        # Linear projection\n",
    "        out = self.out(decoder_out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def get_tgt_mask(self, size : int) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Creates mask to prevent decoder sequences looking\n",
    "        at future characters.\n",
    "        \"\"\"\n",
    "\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Creates mask that masks padding tokens in decoder. \n",
    "        \"\"\"\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        return (matrix == pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSpectroDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 csv_data : str,\n",
    "                 recordings_folder : str='wav_recordings', \n",
    "                 spectr_folder : str='output_logarithmic',\n",
    "                 spectr_format : str='log',\n",
    "                 produce_spectr : bool=False, \n",
    "                 transform : Any=None):\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv_data)\n",
    "        self.transcriptions, self.wav_filenames = self.df['transcription'].tolist(), self.df['channel_1'].tolist()\n",
    "        assert len(self.transcriptions) == len(self.wav_filenames)\n",
    "        \n",
    "        # If we want to generate images from WAV-files (not needed if images are already present)\n",
    "        if produce_spectr:\n",
    "            if spectr_format == 'log':\n",
    "                for name in tqdm(self.wav_filenames):\n",
    "                    if os.path.exists(os.path.join(recordings_folder, name)):\n",
    "                        self._logar(os.path.join(recordings_folder, name), spectr_folder)\n",
    "            \n",
    "            elif spectr_format == 'bw':\n",
    "                for name in tqdm(self.wav_filenames):\n",
    "                    if os.path.exists(os.path.join(recordings_folder, name)):\n",
    "                        self._black_n_white(os.path.join(recordings_folder, name), spectr_folder)\n",
    "\n",
    "            elif spectr_format == 'df':\n",
    "                for name in tqdm(self.wav_filenames):\n",
    "                    if os.path.exists(os.path.join(recordings_folder, name)):\n",
    "                        self._default(os.path.join(recordings_folder, name), spectr_folder)\n",
    "\n",
    "        self.data = []\n",
    "\n",
    "        # Load images and combine with their respective transcriptions \n",
    "        for i in range(len(self.transcriptions)):\n",
    "            transcription = self.transcriptions[i]\n",
    "            fname, _ = self.wav_filenames[i].split('.')\n",
    "            # If check can probably be removed once all WAV-files are processed\n",
    "            if os.path.exists(os.path.join(spectr_folder, fname + '.jpg')):\n",
    "                with Image.open(os.path.join(spectr_folder, fname + '.jpg')) as img_file:\n",
    "                    if self.transform:\n",
    "                        img_tensor = self.transform(img_file)\n",
    "                self.data.append({'img': img_tensor, 'transcription': transcription})\n",
    "       \n",
    "        # Custom tokenizer taken from NorBERT (can try something else as well)\n",
    "        old_tokenizer = AutoTokenizer.from_pretrained('ltg/norbert2')\n",
    "        training_corpus = self.get_all_transcriptions()\n",
    "        self.tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, vocab_size=1200)\n",
    "\n",
    "        new_special_tokens = {'additional_special_tokens' : ['[BOS]', '[EOS]'],}\n",
    "        self.tokenizer.add_special_tokens(new_special_tokens)\n",
    "\n",
    "\n",
    "    def longest_seq(self) -> int:\n",
    "        # + 2 to count for beginning and end\n",
    "        return max([len(self.tokenizer.tokenize(sample['transcription'])) for sample in self.data]) + 2\n",
    "        # return max([len(sample['transcription']) for sample in self.data]) + 2\n",
    "    \n",
    "\n",
    "    def get_vocab_size(self) -> List[str]:\n",
    "        return len(self.tokenizer.vocab)\n",
    "    \n",
    "    \n",
    "    def get_all_transcriptions(self) -> List[dict[torch.tensor, str]]:\n",
    "        transcriptions = [sample['transcription'] for sample in self.data]\n",
    "        return transcriptions\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index : int) -> dict[str, torch.tensor]:\n",
    "        img, seq = self.data[index]['img'], self.data[index]['transcription']\n",
    "        \n",
    "        seq_to_int = [self.tokenizer.vocab['[BOS]']]\n",
    "        seq_to_int.extend(self.tokenizer.encode(seq, add_special_tokens=False))\n",
    "        seq_to_int.append(self.tokenizer.vocab['[EOS]'])\n",
    "\n",
    "        return {'img': img, \n",
    "                'seq': torch.tensor(seq_to_int)}\n",
    "    \n",
    "\n",
    "    def _get_training_corpus(self):\n",
    "        data = self.get_all_transcriptions()\n",
    "        for start_idx in range(0, len(data), 1000):\n",
    "            samples = data[start_idx : start_idx + 1000]\n",
    "            yield samples\n",
    "        \n",
    "\n",
    "    def _logar(self, wav_file_path : str, jpg_folder_output : str) -> None:\n",
    "        y, sr = librosa.load(wav_file_path)\n",
    "        \n",
    "        D = librosa.stft(y)\n",
    "        S = librosa.amplitude_to_db(librosa.magphase(D)[0], ref=np.max)  # different sizes here\n",
    "        file_name = os.path.splitext(os.path.basename(wav_file_path))[0]\n",
    "\n",
    "        output_folder = jpg_folder_output\n",
    "\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        output_path = os.path.join(output_folder, f\"{file_name}.jpg\")\n",
    "\n",
    "        plt.figure(figsize=(7, 3))\n",
    "\n",
    "        librosa.display.specshow(S, y_axis='log', x_axis='time')\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.savefig(output_path, bbox_inches = 'tight', pad_inches = 0)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def _black_n_white(self, file_path):\n",
    "        y, sr = librosa.load(file_path)\n",
    "\n",
    "        D = librosa.stft(y)\n",
    "        S = librosa.amplitude_to_db(librosa.magphase(D)[0], ref=np.max)\n",
    "\n",
    "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "        output_folder = self.spectr_folder\n",
    "\n",
    "\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        output_path = os.path.join(output_folder, f\"{file_name}.jpg\")\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        librosa.display.specshow(S, cmap='gray', y_axis='hz', x_axis='time')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(output_path, bbox_inches = 'tight', pad_inches = 0)\n",
    "\n",
    "\n",
    "    def _default(self, file_path):\n",
    "        y, sr = librosa.load(file_path)\n",
    "\n",
    "        D = librosa.stft(y)\n",
    "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "        output_folder = self.spectr_folder\n",
    "\n",
    "\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        output_path = os.path.join(output_folder, f\"{file_name}.jpg\")\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max), y_axis='hz', x_axis='time')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(output_path, bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateFunctor:\n",
    "    \"\"\"\n",
    "    Simple collator to pad decoder sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pad_idx : int) -> None:\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    def __call__(self, samples) -> dict[str, torch.tensor]:\n",
    "        img_tensors = []\n",
    "        sequences = []\n",
    "\n",
    "        for sample in sorted(samples, key=lambda x: len(x['seq']), reverse=True):\n",
    "            img_tensors.append(sample['img'])\n",
    "            sequences.append(sample['seq'])\n",
    "\n",
    "        # Padding sequences\n",
    "        padded_seq_tensors = pad_sequence(sequences, \n",
    "                                          batch_first=True,\n",
    "                                          padding_value=self.pad_idx)\n",
    "        \n",
    "        img_tensors = torch.stack(tuple(img_tensors), dim=0)\n",
    "        return {'img': img_tensors,\n",
    "                'seq': padded_seq_tensors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset variables \n",
    "data_path = 'dataset.csv' \n",
    "produce_spectr = False\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((231, 540)),  # RESIZE SHOULD BE INCLUDED ONLY IF PATCH SHAPE DOESN'T MATCH IMAGE SHAPE\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = CustomSpectroDataset(csv_data=data_path,\n",
    "                               produce_spectr=produce_spectr,\n",
    "                               transform=transform,\n",
    "                               spectr_folder='log_rectangular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test splits\n",
    "seed = 7\n",
    "gen = torch.Generator().manual_seed(seed)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.9, 0.1], generator=gen)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=32, \n",
    "                              shuffle=True,\n",
    "                              collate_fn=CollateFunctor(pad_idx=dataset.tokenizer.pad_token_id))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                             batch_size=32, \n",
    "                             shuffle=True,\n",
    "                             collate_fn=CollateFunctor(dataset.tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Model hyperparameters\n",
    "patch_size = (231, 54) \n",
    "dim_model = 240\n",
    "encoder_layers = 4\n",
    "decoder_layers = 4\n",
    "heads = 6\n",
    "mlp_dim = 1024\n",
    "dropout_p = 0.1\n",
    "\n",
    "vocab_size = dataset.get_vocab_size()\n",
    "max_seq_len = dataset.longest_seq()\n",
    "n_channels = 3 \n",
    "img_shape = (3, 231, 540)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Img2SeqModel(device=device,\n",
    "                     patch_size=patch_size,\n",
    "                     dim_model=dim_model,\n",
    "                     encoder_layers=encoder_layers,\n",
    "                     decoder_layers=decoder_layers,\n",
    "                     heads=heads,\n",
    "                     mlp_dim=mlp_dim,\n",
    "                     dropout_p=dropout_p,\n",
    "                     vocab_size=vocab_size,\n",
    "                     max_seq_len=max_seq_len,\n",
    "                     img_shape=img_shape )\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "epochs = 3\n",
    "init_lr = 1e-5\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=init_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, img, device):\n",
    "    iterations = 10\n",
    "    \"\"\"\n",
    "    Generates text based on sepctrogram, BOS-token + potentially some other token of choice.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    seq_inp = torch.tensor([dataset.tokenizer.vocab['[BOS]']], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Iterations is arbitrary - can be changed to any number\n",
    "    for _ in range(iterations):\n",
    "        logits = model(img, seq_inp)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1).detach().to('cpu')\n",
    "        idx_next = np.argmax(probs, axis=-1).unsqueeze(0).to(device)\n",
    "        seq_inp = torch.cat(((seq_inp, idx_next)), dim=1)\n",
    "    \n",
    "    print(dataset.tokenizer.decode(seq_inp.squeeze(0).detach().to('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train loop\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    eval_batches = [50, 100, 150, 200, 250]\n",
    "    for batch_num, batch in enumerate(tqdm(iterator), 1):\n",
    "        img = batch['img'].to(device)\n",
    "        target_seq = batch['seq'].to(device)\n",
    "\n",
    "        sequence_len = target_seq.size(1)\n",
    "        tgt_attention_mask = model.get_tgt_mask(sequence_len).to(device)\n",
    "        tgt_pad_mask = model.create_pad_mask(target_seq, dataset.tokenizer.pad_token_id)\n",
    "\n",
    "        logits = model(img, target_seq, tgt_mask=tgt_attention_mask, tgt_pad_mask=tgt_pad_mask)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        target_seq = target_seq.view(B*T)\n",
    "        loss = F.cross_entropy(logits, target_seq, ignore_index=dataset.tokenizer.pad_token_id)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "        # Test on a single spectrogram\n",
    "        if batch_num in eval_batches:\n",
    "            single_img = img[0].unsqueeze(0)\n",
    "            generate(model, single_img, device)\n",
    "    \n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {epoch + 1}', end='\\n\\n')\n",
    "    epoch_loss = train(model, train_dataloader, optimizer, device)\n",
    "    print()\n",
    "    print(f'Loss: {epoch_loss}')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spectrorgram_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
